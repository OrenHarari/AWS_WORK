


    1  pwd
    2  docker pull redis
    3  docker pull redis
    4  docker run -d -p 6379:6379 --name redis1 redis
    5  #!/bin/bash
    6  # Source: https://repost.aws/articles/ARwfQMxiC-QMOgWykD9mco1w/how-do-i-install-nvidia-gpu-driver-cuda-toolkit-and-optionally-nvidia-container-to
olkit-on-amazon-linux-2023-al2023
    7  # To verify installation status, connect to your EC2 instance and view /var/log/cloud-init-output.log contents
    8  sudo dnf update -y
    9  sudo dnf install -y dkms kernel-devel kernel-modules-extra
   10  cd /tmp
   11  sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/amzn2023/x86_64/cuda-amzn2023.repo
   12  sudo dnf clean expire-cache
   13  sudo dnf module install -y nvidia-driver:latest-dkms
   14  sudo dnf install -y cuda-toolkit
   15  nvidia-smi
   16  /usr/local/cuda/bin/nvcc -V
   17  sudo dnf config-manager --add-repo https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo
   18  sudo dnf clean expire-cache
   19  sudo dnf install -y nvidia-container-toolkit
   20  nvidia-container-cli -V
   21  sudo dnf install -y docker
   22  sudo usermod -aG docker ec2-user
   23  sudo nvidia-ctk runtime configure --runtime=docker
   24  sudo systemctl restart docker
   25  sudo docker run --rm --runtime=nvidia --gpus all public.ecr.aws/amazonlinux/amazonlinux:2023 nvidia-smi
   26  sudo reboot
   27  #!/bin/bash
   28  # Source: https://repost.aws/articles/ARwfQMxiC-QMOgWykD9mco1w/how-do-i-install-nvidia-gpu-driver-cuda-toolkit-and-optionally-nvidia-container-to
olkit-on-amazon-linux-2023-al2023
   29  # To verify installation status, connect to your EC2 instance and view /var/log/cloud-init-output.log contents
   30  sudo dnf update -y
   31  sudo dnf install -y dkms kernel-devel kernel-modules-extra
   32  cd /tmp
   33  sudo dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/amzn2023/x86_64/cuda-amzn2023.repo
   34  sudo dnf clean expire-cache
   35  sudo dnf module install -y nvidia-driver:latest-dkms
   36  sudo dnf install -y cuda-toolkit
   37  nvidia-smi
   38  /usr/local/cuda/bin/nvcc -V
   39  sudo dnf config-manager --add-repo https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo
   40  sudo dnf clean expire-cache
   41  sudo dnf install -y nvidia-container-toolkit
   42  nvidia-container-cli -V
   43  sudo dnf install -y docker
   44  sudo usermod -aG docker ec2-user
   45  sudo nvidia-ctk runtime configure --runtime=docker
   46  sudo systemctl restart docker
   47  sudo docker run --rm --runtime=nvidia --gpus all public.ecr.aws/amazonlinux/amazonlinux:2023 nvidia-smi
   48  sudo reboot




   [ec2-user@ip-10-0-0-31 ~]$ history
    1  cat /var/log/cloud-init-output.log
    2  whoami
    3  python
    4  sudo yum install python
    5  sudo yum update
    6  sudo yum install python
    7  pip
    8  python
    9  sudo yum install -y python-pip
   10  pip --version
   11  pip install huggingfacecli
   12  pip install huggingface_hub
   13  pip install huggingface_cli
   14  pip install huggingface_cli login
   15  huggingface-cli login
   16  df -h
   17  vi index.html
   18  ll
   19  cat index.html
   20  python -m http.server 443
   21  vi index.html
   22  chmod 777 index.html
   23  ll
   24  python -m http.server 443
   25  sudo python -m http.server 443
   26  pwd
   27  ll
   28  docker network create llm-network
   29  docker run -d   --name llm   --runtime nvidia   --gpus all   --network llm-network   -v ~/.cache/huggingface:/root/.cache/huggingface   --env "HUGGING_FACE_HUB_TOKEN=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"   -p 80:8000   --ipc=host   vllm/vllm-openai:latest   --model mistralai/Mistral-7B-Instruct-v0.3   --tokenizer-mode auto   --tensor-parallel-size 1   --gpu-memory-utilization 0.85   --max-model-len 4096   --served-model-name gpt-3.5-turbo
   30  docker logs -f llm
   31  docker rm llm
   32  docker run -d   --name llm   --runtime nvidia   --gpus all   --network llm-network   -v ~/.cache/huggingface:/root/.cache/huggingface   --env "HUGGING_FACE_HUB_TOKEN=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"   -p 80:8000   --ipc=host   vllm/vllm-openai:latest   --model mistralai/Mistral-7B-Instruct-v0.3   --tokenizer-mode auto   --tensor-parallel-size 1   --gpu-memory-utilization 0.85   --max-model-len 4096   --served-model-name gpt-3.5-turbo
   33  docker logs -f llm
   34  docker rm llm
   35  docker network create llm-network
   36  docker run -d   --name llm   --runtime nvidia   --gpus all   --network llm-network   -v ~/.cache/huggingface:/root/.cache/huggingface   --env "HUGGING_FACE_HUB_TOKEN=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"   -p 80:8000   --ipc=host   vllm/vllm-openai:latest   --model mistralai/Mistral-7B-Instruct-v0.3   --tokenizer-mode auto   --tensor-parallel-size 1   --gpu-memory-utilization 0.85   --max-model-len 4096   --served-model-name gpt-3.5-turbo
   37  docker logs -f llm
   38  docker run -d   --name chatui   --network llm-network   -p 443:3000   -e OPENAI_API_HOST=http://llm:8000   -e OPENAI_API_KEY=dummy   -e DEFAULT_MODEL=gpt-3.5-turbo   ghcr.io/mckaywrigley/chatbot-ui:main
